#### 监督学习解决的问题
监督学习 在于学习两个数据集的联系：观察数据 X 和我们正在尝试预测的额外变量 y (通常称“目标”或“标签”)， 而且通常是长度为 n_samples 的一维数组。

scikit-learn 中所有监督的 估计量 <https://en.wikipedia.org/wiki/Estimator> 都有一个用来拟合模型的 fit(X, y) 方法，和根据给定的没有标签观察值 X 返回预测的带标签的 y 的 predict(X) 方法。

#### 词汇：分类和回归
如果预测任务是为了将观察值分类到有限的标签集合中，换句话说，就是给观察对象命名，那任务就被称为 分类 任务。另外，如果任务是为了预测一个连续的目标变量，那就被称为 回归 任务。

当在 scikit-learn 中进行分类时，y 是一个整数或字符型的向量。

***
## 最近邻和维度惩罚

```python
import numpy as np
from sklearn import datasets

iris=datasets.load_iris()
iris_x=iris.data
iris_y=iris.target

#显示所以y 的分类
print(np.unique(iris_y))
"""
[0 1 2]
"""

#将iris分为测试集和训练集
np.random.seed(0)
#随机排列，用于使分解的数据随机分布
indices=np.random.permutation(len(iris_x))

#训练数据
iris_x_train=iris_x[indices[:-10]]
iris_y_train=iris_y[indices[:-10]]

#测试数据
iris_x_test=iris_x[indices[-10:]]
iris_y_test=iris_y[indices[-10:]]


#创建和拟合一个knn
from sklearn.neighbors import KNeighborsClassifier
knn=KNeighborsClassifier()
print(knn.fit(iris_x_train,iris_y_train))
"""
KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=1, n_neighbors=5, p=2,
           weights='uniform')
"""

print(knn.predict(iris_x_test))
print(iris_y_test)
"""
[1 2 1 0 0 0 2 1 2 0]
[1 1 1 0 0 0 2 1 2 0]
"""


```
***
## 线性模型：从回归到稀疏

#### 糖尿病数据集
```python
#糖尿病数据集
diabetes=datasets.load_diabetes()

diabetes_x_train=diabetes.data[:-20]
diabetes_y_train=diabetes.target[:-20]

diabetes_x_test=diabetes.data[-20:]
diabetes_y_test=diabetes.target[-20:]
```


#### 线性回归

```python
#线性回归
from sklearn import linear_model

regr=linear_model.LinearRegression()
print(regr.fit(diabetes_x_train,diabetes_y_train))
"""
LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
"""

#回归系数
print(regr.coef_)
"""
[  3.03499549e-01  -2.37639315e+02   5.10530605e+02   3.27736980e+02
  -8.14131709e+02   4.92814588e+02   1.02848452e+02   1.84606489e+02
   7.43519617e+02   7.60951722e+01]
"""


#均方误差,其中mean表示求平均值
print(np.mean((regr.predict(diabetes_x_test)-diabetes_y_test)**2))
"""
2004.56760269
"""

#方差分数：1表示完美预测，0表示没有任何关系
print(regr.score(diabetes_x_test,diabetes_y_test))
"""
0.585075302269
"""
```

#### 收缩
如果每个维度的数据点很少，观察噪声就会导致很大的方差：

```python
#c_表示按列进行连接
X=np.c_[.5,1].T
y=[.5,1]
test=np.c_[0,2].T
regr=linear_model.LinearRegression()

import matplotlib.pyplot as plt
plt.figure()
np.random.seed(0)
for _ in range(6):
    this_X = .1*np.random.normal(size=(2, 1)) + X
    regr.fit(this_X, y)
    plt.plot(test, regr.predict(test)) 
    plt.scatter(this_X, y, s=3)     
    
```

高纬统计学习中的一个解决方法是 收缩 回归系数到0：任何两个随机选择的观察值数据集都很可能是不相关的。这称为 岭回归 ：
```python 

regr = linear_model.Ridge(alpha=.1)

plt.figure() 

np.random.seed(0)
for _ in range(6): 
    this_X = .1*np.random.normal(size=(2, 1)) + X
    regr.fit(this_X, y)
    plt.plot(test, regr.predict(test)) 
    plt.scatter(this_X, y, s=3) 

```
#### 稀疏
为了提高问题的条件(比如，缓解`维度惩罚`)，只选择信息特征和设置无信息时就会变得有趣，比如特征2到0。岭回归会减小他们的值，但不会减到0.另一种抑制方法，称为 Lasso (最小绝对收缩和选择算子)，可以把一些系数设为0。这些方法称为 稀疏法，稀疏可以看作是奥卡姆剃刀的应用：模型越简单越好。

```python
regr = linear_model.Lasso()
scores = [regr.set_params(alpha=alpha
            ).fit(diabetes_X_train, diabetes_y_train
            ).score(diabetes_X_test, diabetes_y_test)
       for alpha in alphas]
best_alpha = alphas[scores.index(max(scores))]
regr.alpha = best_alpha
regr.fit(diabetes_X_train, diabetes_y_train)



print(regr.coef_)
```





#### 分类
```python
#分类
#尝试用最近邻和线性模型分类数字数据集。留出最后 10%的数据，并测试观察值预期效果

from sklearn import datasets, neighbors, linear_model

digits = datasets.load_digits()
X_digits = digits.data
y_digits = digits.target

n_samples = len(X_digits)

X_train = X_digits[:int(.9 * n_samples)]
y_train = y_digits[:int(.9 * n_samples)]
X_test = X_digits[int(.9 * n_samples):]
y_test = y_digits[int(.9 * n_samples):]

knn = neighbors.KNeighborsClassifier()
logistic = linear_model.LogisticRegression()

print('KNN score: %f' % knn.fit(X_train, y_train).score(X_test, y_test))
print('LogisticRegression score: %f' % logistic.fit(X_train, y_train).score(X_test, y_test))
"""
KNN score: 0.961111
LogisticRegression score: 0.938889
"""
```


***
## 支持向量机（SVMs）

#### 线性 SVMs
SVMs 可以用于回归 –:class: SVR (支持向量回归)–，或者分类 –:class: SVC (支持向量分类)。

```python
#线性svms

from sklearn import svm
svc=svm.SVC(kernel='linear')
svc.fit(iris_x_train,iris_y_train)
print(svc)
"""
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False)
"""
```


#### 使用核

```python

#根据特征1和特征2，尝试用 SVMs 把1和2类从鸢尾属植物数据集中分出来。
#为每一个类留下10%，并测试这些观察值预期效果。
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets, svm

iris = datasets.load_iris()
X = iris.data
y = iris.target

X = X[y != 0, :2]
y = y[y != 0]

n_sample = len(X)

np.random.seed(0)
order = np.random.permutation(n_sample)
X = X[order]
y = y[order].astype(np.float)

X_train = X[:int(.9 * n_sample)]
y_train = y[:int(.9 * n_sample)]
X_test = X[int(.9 * n_sample):]
y_test = y[int(.9 * n_sample):]

# fit the model
for fig_num, kernel in enumerate(('linear', 'rbf', 'poly')):
    clf = svm.SVC(kernel=kernel, gamma=10)
    clf.fit(X_train, y_train)

    plt.figure(fig_num)
    plt.clf()
    plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired,
                edgecolor='k', s=20)

    # Circle out the test data
    plt.scatter(X_test[:, 0], X_test[:, 1], s=80, facecolors='none',
                zorder=10, edgecolor='k')

    plt.axis('tight')
    x_min = X[:, 0].min()
    x_max = X[:, 0].max()
    y_min = X[:, 1].min()
    y_max = X[:, 1].max()

    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]
    Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])

    # Put the result into a color plot
    Z = Z.reshape(XX.shape)
    plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)
    plt.contour(XX, YY, Z, colors=['k', 'k', 'k'],
                linestyles=['--', '-', '--'], levels=[-.5, 0, .5])

    plt.title(kernel)
plt.show()
```




  